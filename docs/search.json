[
  {
    "objectID": "posts/2020-04-26-Normalization.html",
    "href": "posts/2020-04-26-Normalization.html",
    "title": "Normalization",
    "section": "",
    "text": "Normalization in Deep Learning\nNormalization is an important technique widely used in Deep Learning to achieve better results in less time.\n\nWhy do we need to Normalize in the first place?\nCovariate Shift: Most of the time the training dataset is very different from the real dataset. Suppose, a CNN model is trained to classify cats. But the training dataset only had images of black cats. So, if the model is fed with an image of a white cat it may not predict correctly. This phenomenon is called Covariate-Shift.\nIn the graph below, the training data mostly falls on a linear function. But after including test data it looks more like a quadratic distribution.\nSource: http://iwann.ugr.es/2011/pdf/InvitedTalk-FHerrera-IWANN11.pdf\nGenerally in image datasets, the distribution can change because of change in camera resolutions or any other environmental change."
  },
  {
    "objectID": "posts/2020-01-13-EfficientDet.html",
    "href": "posts/2020-01-13-EfficientDet.html",
    "title": "EfficientDet: When Object Detection Meets Scalability and Efficiency",
    "section": "",
    "text": "EfficientDet, a highly efficient and scalable state of the art object detection model developed by Google Research, Brain Team. It is not just a single model. It has a family of detectors which achieve better accuracy with an order-of-magnitude fewer parameters and FLOPS than previous object detectors.\nEfficientDet paper has mentioned its 7 family members.\nSource: arXiv:1911.09070v1"
  },
  {
    "objectID": "posts/2020-01-13-EfficientDet.html#quick-overview-of-the-paper",
    "href": "posts/2020-01-13-EfficientDet.html#quick-overview-of-the-paper",
    "title": "EfficientDet: When Object Detection Meets Scalability and Efficiency",
    "section": "Quick Overview of the Paper",
    "text": "Quick Overview of the Paper\n\nEfficientNet is the backbone architecture used in the model. EfficientNet is also written by the same authors at Google. Conventional CNN models arbitrarily scaled network dimensions- width, depth and resolution. EfficientNet uniformly scales each dimension with a fixed set of scaling coefficients. It surpassed SOTA accuracy with 10x efficiency.\nBiFPN: While fusing (applying residual or skip connections) different input features, most of the works simply summed them up without any distinction. Since both input features are at the different resolutions they don’t equally contribute to the fused output layer. The paper proposes a weighted bi-directional feature pyramid network (BiFPN), which introduces learnable weights to learn the importance of different input features.\nCompound Scaling: For higher accuracy previous object detection models relied on — bigger backbone or larger input image sizes. Compound Scaling is a method that uses a simple compound coefficient φ to jointly scale-up all dimensions of the backbone network, BiFPN network, class/box network, and resolution.\n\n\nCombining EfficientNet backbones with our propose BiFPN and compound scaling, we have developed a new family of object detectors, named EfficientDet, which consistently achieve better accuracy with an order-of-magnitude fewer parameters and FLOPS than previous object detectors."
  },
  {
    "objectID": "posts/2020-01-13-EfficientDet.html#bifpn",
    "href": "posts/2020-01-13-EfficientDet.html#bifpn",
    "title": "EfficientDet: When Object Detection Meets Scalability and Efficiency",
    "section": "BiFPN",
    "text": "BiFPN\nSource: arXiv:1911.09070v1 — figure 2\nConventional FPN (Feature Pyramid Network) is limited by the one-way information flow. PANet added an extra bottom-up path for information flow. PANet achieved better accuracy but with the cost and more parameters and computations. The paper proposed several optimizations for cross-scale connections:\n\nRemove Nodes that only have one input edge. If a node has only one input edge with no feature fusion, then it will have less contribution to the feature network that aims at fusing different features.\nAdd an extra edge from the original input to output node if they are at the same level, in order to fuse more features without adding much cost.\nTreat each bidirectional (top-down & bottom-up) path as one feature network layer, and repeat the same layer multiple times to enable more high-level feature fusion."
  },
  {
    "objectID": "posts/2020-01-13-EfficientDet.html#weighted-feature-fusion",
    "href": "posts/2020-01-13-EfficientDet.html#weighted-feature-fusion",
    "title": "EfficientDet: When Object Detection Meets Scalability and Efficiency",
    "section": "Weighted Feature Fusion",
    "text": "Weighted Feature Fusion\nWhile multi-scale fusion, input features are not simply summed up. The authors proposed to add additional weight for each input during feature fusion and let the network to learn the importance of each input feature. Out of three weighted fusion approaches — Unbounded fusion:\nSource: https://arxiv.org/abs/1911.09070\nWhere W is a learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel). Since the scalar weight is unbounded, it could potentially cause training instability. So, Softmax-based fusion was tried for normalized weights. Softmax-based fusion:\n\n\n\nSource: https://arxiv.org/abs/1911.09070\n\n\nAs softmax normalizes the weights to be the probability of range 0 to 1 which can denote the importance of each input. The softmax leads to a slowdown on GPU. Fast normalized fusion:\n\n\n\nSource: https://arxiv.org/abs/1911.09070\n\n\nЄ is added for numeric stability. It is 30% faster on GPU and gave almost as accurate results as softmax. > Final BiFPN integrates both the bidirectional cross-scale connections and the fast normalized fusion."
  },
  {
    "objectID": "posts/2020-01-13-EfficientDet.html#efficientdet-architecture",
    "href": "posts/2020-01-13-EfficientDet.html#efficientdet-architecture",
    "title": "EfficientDet: When Object Detection Meets Scalability and Efficiency",
    "section": "EfficientDet Architecture",
    "text": "EfficientDet Architecture\nSource: arXiv:1911.09070v1 — figure 3\nEfficientDet follows one-stage-detection paradigm. A pre-trained EfficientNet backbone is used with BiFPN as the feature extractor. BiFPNN takes {P3, P4, P5, P6, P7} features from the EfficientNet backbone network and repeatedly applies bidirectional feature fusion. The fused features are fed to a class and bounding box network for predicting object class and bounding box."
  },
  {
    "objectID": "posts/2020-01-13-EfficientDet.html#references",
    "href": "posts/2020-01-13-EfficientDet.html#references",
    "title": "EfficientDet: When Object Detection Meets Scalability and Efficiency",
    "section": "References",
    "text": "References\nEfficientDet: Scalable and Efficient Object Detection\nEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\nPath Aggregation Network for Instance Segmentation\nDeep Residual Learning for Image Recognition\n\n > Hope you liked the article.\n👉 Twitter: https://twitter.com/aniketmaurya 👉 Mail: aniketmaurya@outlook.com"
  },
  {
    "objectID": "posts/2020-11-16-dcgan.html#discriminator",
    "href": "posts/2020-11-16-dcgan.html#discriminator",
    "title": "DCGAN Tutorial - Generate Fake Celebrity image",
    "section": "Discriminator",
    "text": "Discriminator\nThe architecture of a Discriminator is same as that of a normal image classification model. It contains Convolution layers, Activation layer and BatchNormalisation. In the DCGAN paper, strides are used instead of pooling to reduce the size of a kernel. Also there is no Fully Connected layer in the network. Leaky ReLU with leak slope 0.2 is used.\nThe Discriminator wants to predict the fake images as fake and real images as real. On the other hand the Generator wants to fool Discriminator into predicting the fake images produced by the Generator as real.\n\n\n\n\n\n\n\n\n\n\n\nSource: deeplearning.ai GANs Specialisation\n\n\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, im_chan=3, hidden_dim=32):\n        super(Discriminator, self).__init__()\n        self.disc = nn.Sequential(\n            self.make_disc_block(im_chan, hidden_dim),\n            self.make_disc_block(hidden_dim, hidden_dim * 2),\n            self.make_disc_block(hidden_dim*2, hidden_dim * 4, stride=1),\n            self.make_disc_block(hidden_dim*4, hidden_dim * 4, stride=2),\n            self.make_disc_block(hidden_dim * 4, 1, final_layer=True),\n        )\n\n    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.LeakyReLU(0.2)\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n            )\n\n    def forward(self, image):\n        disc_pred = self.disc(image)\n        return disc_pred.view(len(disc_pred), -1)\n\nDefine learning rate, z_dim (noise dimension), batch size and other configuration based on the paper.\n\n\nCode\n# Configurations are from DCGAN paper\nz_dim = 100\nbatch_size = 128\nlr = 0.0002\n\nbeta_1 = 0.5 \nbeta_2 = 0.999\ndevice = 'cuda'\n\n\n\n\nCode\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n\ngen = Generator().to(device)\ndisc = Discriminator().to(device)\n\n\ngen_optimizer = Adam(gen.parameters(), lr, betas=(beta_1, beta_2))\ndisc_optimizer = Adam(disc.parameters(), lr, betas=(beta_1, beta_2))\n\n\ngen = gen.apply(weights_init)\ndisc = disc.apply(weights_init)\n\n\n\n\nCode\n# You can tranform the image values to be between -1 and 1 (the range of the tanh activation)\ntransform=transforms.Compose([\n                            transforms.Resize(64),\n                            transforms.CenterCrop(64),\n                            transforms.ToTensor(),\n                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                        ])\n\ndataloader = DataLoader(\n    datasets.CelebA('.', download=True, transform=transform),\n    batch_size=batch_size,\n    shuffle=True)"
  },
  {
    "objectID": "posts/2020-11-16-dcgan.html#discriminator-loss",
    "href": "posts/2020-11-16-dcgan.html#discriminator-loss",
    "title": "DCGAN Tutorial - Generate Fake Celebrity image",
    "section": "Discriminator Loss",
    "text": "Discriminator Loss\nAs the discriminator wants to increase the distance between Generated and Real distribution, we will train it to give high loss when the generated images is classified as real or when real images are classified as fake."
  },
  {
    "objectID": "posts/2020-11-16-dcgan.html#generator-loss",
    "href": "posts/2020-11-16-dcgan.html#generator-loss",
    "title": "DCGAN Tutorial - Generate Fake Celebrity image",
    "section": "Generator Loss",
    "text": "Generator Loss\nThe BCE loss for Generator will be high when it fails to fool the Discriminator. It will give high loss when the generated image is classified as fake by the discriminator. Note that the Generator never know about real images.\n\ncriterion = nn.BCEWithLogitsLoss()\ndisplay_step = 500\n\n\nn_epochs = 50\ncur_step = 0\nmean_generator_loss = 0\nmean_discriminator_loss = 0\nfor epoch in range(n_epoch):\n    for real, _ in tqdm(dataloader):\n        real = real.to(device)\n\n\n        # update the discriminator\n        # create fake images from random noise\n        disc_optimizer.zero_grad()\n        noise = torch.randn(cur_batch_size, z_dim, 1, 1, device=device)\n        fake_images = gen(noise)\n        logits_fake = disc(fake_images.detach())\n        logits_real = disc(real)\n\n        disc_loss_fake = criterion(fake_logits, torch.zeros_like(loss_fake))\n        disc_loss_real = criterion(real_logits, torch.ones_like(logits_real))\n\n        disc_loss = (disc_loss_fake + disc_loss_real) / 2\n        # Keep track of the average discriminator loss\n        mean_discriminator_loss += disc_avg_loss.item() / display_step\n\n        disc_loss.backward(retain_graph=True)\n        disc_optimizer.step()\n\n\n        # Update the generator\n        gen_optimizer.zero_grad()\n        noise = torch.randn(cur_batch_size, z_dim, 1, 1, device=device)\n        fake_images = gen(noise)\n        logits_fake = disc(fake_images)\n\n        gen_loss = criterion(logits_fake, torch.ones_like(logits_fake))\n        gen_loss.backward()\n        gen_optimizer.step()\n\n        # Keep track of the average generator loss\n        mean_generator_loss += gen_loss.item() / display_step\n\n        ## Visualization code ##\n        if cur_step % display_step == 0 and cur_step > 0:\n            print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n            show_tensor_images(fake_images)\n            show_tensor_images(real)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1"
  },
  {
    "objectID": "posts/2020-03-27-linear regression scratch.html",
    "href": "posts/2020-03-27-linear regression scratch.html",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2020-03-27-linear regression scratch.html#data",
    "href": "posts/2020-03-27-linear regression scratch.html#data",
    "title": "Linear Regression from Scratch",
    "section": "Data",
    "text": "Data\n\nX = [*range(1, 51)]\nY = list(map(lambda x: 2*x + 5, X))"
  },
  {
    "objectID": "posts/2020-03-27-linear regression scratch.html#univariate-regression",
    "href": "posts/2020-03-27-linear regression scratch.html#univariate-regression",
    "title": "Linear Regression from Scratch",
    "section": "Univariate Regression",
    "text": "Univariate Regression\n\\(h() = X + b\\)\n\nMSE cost function\n\\((h(x) - y)^2\\)\n\n\nGradient Descent\n\nrepeat {\n\n    Ø = Ø - ∆J(Ø) = Ø - LR*1/m * sum((h(Ø, b) - Y)*X)    \n    \n    b = b - ∆J(b) =  b - LR*1/m * sum((h(Ø, b) - Y))\n}\n\n\ndef mse(y_true, y_pred):\n    cost = 0\n    m = len(y_pred)\n    for i in range(m):\n        cost += (y_pred[i] - y_true[i]) ** 2\n    return cost/(2*m)\n\ndef der_mse(y_true, y_pred):\n    der_cost = 0\n    m = len(y_pred)\n    for i in range(m):\n        der_cost += (y_pred[i] - y_true[i])\n    return der_cost\n\ndef predict(x):\n    return w*x + b\n\n\n# Intialization of variables\n\nm = len(X)\nLR = 0.01\nw,b =0,0.1\n\nepochs = 10000\n# Training\n\ntotal_cost = []\nfor i in range(epochs):\n    y_pred = []\n    epoch_cost = []\n    for num, data in enumerate(zip(X, Y)):\n        x, y = data\n        y_pred = []\n        y_pred.append(w*x + b)\n        \n        \n        cost = mse(Y[num:num+1], y_pred)\n        epoch_cost.append(cost)\n        der_cost = der_mse(Y[num:num+1], y_pred)\n\n        w -= LR * (1/m) * der_cost * x\n        b -= LR * (1/m) * der_cost\n        \n    total_cost.append(np.mean(epoch_cost))\n    if i%500==0:\n        print(f'epoch:{i}\\t\\tcost:{cost}')\n\nepoch:0     cost:0.024546020195931887\nepoch:500       cost:0.0035238913511105277\nepoch:1000      cost:0.0004771777468473895\nepoch:1500      cost:6.461567040474519e-05\nepoch:2000      cost:8.749747634800157e-06\nepoch:2500      cost:1.1848222450189964e-06\nepoch:3000      cost:1.604393419109384e-07\nepoch:3500      cost:2.1725438173628743e-08\nepoch:4000      cost:2.9418885555175706e-09\nepoch:4500      cost:3.983674896607656e-10\nepoch:5000      cost:5.3943803161575866e-11\nepoch:5500      cost:7.30464704919418e-12\nepoch:6000      cost:9.891380608202818e-13\nepoch:6500      cost:1.3394131683086816e-13\nepoch:7000      cost:1.8137281109430194e-14\nepoch:7500      cost:2.4560089530711338e-15\nepoch:8000      cost:3.3257381016463754e-16\nepoch:8500      cost:4.5034718706313674e-17\nepoch:9000      cost:6.09814092196085e-18\nepoch:9500      cost:8.25761584212193e-19\n\n\n\npredict(2), predict(9)\n\n(8.999999990490096, 22.999999991911498)\n\n\n\nw, b\n\n(2.000000000203057, 4.999999990083981)\n\n\n\nplt.plot(total_cost)\nplt.show()"
  },
  {
    "objectID": "posts/2020-04-29-Probability-vs-Likelihood.html#joint-probability",
    "href": "posts/2020-04-29-Probability-vs-Likelihood.html#joint-probability",
    "title": "Probability vs Likelihood vs Maximum Likelihood",
    "section": "Joint Probability",
    "text": "Joint Probability\nJoint probability is the probability of two events occurring simultaneously. Rolling two dice together is an example of Joint Probability.\nIf the two events E1 and E2 are independent of each other then the joint probability is multiplication of P(E1) and P(E2).\n\\(P(E_1\\ and\\ E_2) = P(E_1) \\times P(E_2)\\)\nP(A given B) is denoted as P(A|B)\nIf the two events E1 and E2 are not independent then the joint probability is:\n\\(P(E_1\\ and\\ E_2) = P(E_1|E_2)P(E_2) = P(E_2|E_1)P(E_1)\\)\n\nJoint probability is symmetric\nP(A,B) = P(B, A)\n\nP(A,B) = P(B|A)P(A)\n\nP(B,A) = P(A|B)P(B)\n\nP(A|B)P(B) = P(B|A)P(A)\n\\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\)"
  },
  {
    "objectID": "posts/2020-04-29-Probability-vs-Likelihood.html#conditional-probability",
    "href": "posts/2020-04-29-Probability-vs-Likelihood.html#conditional-probability",
    "title": "Probability vs Likelihood vs Maximum Likelihood",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nIt is the probability of one event when occurence of the other is given.\nConsider two events A and B, given that B has occurred. Then the probability of A given B is:\n\\(P(A\\|B) = \\frac{P(B\\|A)P(B)}{P(A)} = \\frac{P(A, B)}{P(A)}\\)"
  },
  {
    "objectID": "posts/2020-04-29-Probability-vs-Likelihood.html#maximum-likelihood",
    "href": "posts/2020-04-29-Probability-vs-Likelihood.html#maximum-likelihood",
    "title": "Probability vs Likelihood vs Maximum Likelihood",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nMaximum Likelihood is used to find the normal distribution of the given data. We estimate \\(\\mu\\) and \\(\\sigma\\) for the distribution."
  },
  {
    "objectID": "posts/2022-04-03-happywhale-pytorch-training-from-scratch-lite.html",
    "href": "posts/2022-04-03-happywhale-pytorch-training-from-scratch-lite.html",
    "title": "HappyWhale 🐳:PyTorch Training from scratch Lite ⚡️",
    "section": "",
    "text": "In this Notebook article, you will learn how to write a custom training loop in pure PyTorch, create custom torch Dataset class, compute metrics for model performance, and Scale the Training on any hardware like GPU, TPU, IPU or Distributed Training with LightningLite.\nCheckout the original Kaggle Notebook here."
  },
  {
    "objectID": "posts/2022-04-03-happywhale-pytorch-training-from-scratch-lite.html#browse-some-images",
    "href": "posts/2022-04-03-happywhale-pytorch-training-from-scratch-lite.html#browse-some-images",
    "title": "HappyWhale 🐳:PyTorch Training from scratch Lite ⚡️",
    "section": "Browse some images",
    "text": "Browse some images\n\nnb_species = len(df_train[\"species\"].unique())\nfig, axarr = plt.subplots(ncols=5, nrows=nb_species, figsize=(12, nb_species * 2))\n\nfor i, (name, dfg) in enumerate(df_train.groupby(\"species\")):\n    axarr[i, 0].set_title(name)\n    for j, (_, row) in enumerate(dfg[:5].iterrows()):\n        im_path = os.path.join(PATH_DATASET, \"train_images\", row[\"image\"])\n        img = plt.imread(im_path)\n        axarr[i, j].imshow(img)\n        axarr[i, j].set_axis_off()\n\n\n\n\n\n# !pip install -q -U timm pytorch-lightning>=1.6\n\n\nimport torch\n\nimport timm\nfrom PIL import Image\nfrom torchvision import transforms as T\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n\nfrom torchmetrics import F1\nimport torch.nn.functional as F\n\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch.optim.lr_scheduler import StepLR"
  },
  {
    "objectID": "posts/2022-04-03-happywhale-pytorch-training-from-scratch-lite.html#training-loop",
    "href": "posts/2022-04-03-happywhale-pytorch-training-from-scratch-lite.html#training-loop",
    "title": "HappyWhale 🐳:PyTorch Training from scratch Lite ⚡️",
    "section": "Training Loop",
    "text": "Training Loop\nFor writing the training loop we will iterate a for loop for given number of epochs num_epochs. Set the model to training mode with model.train() and iterate through the dataloader. We pass the data to model and calculate the crossentropy loss. We do loss.backward() to compute gradients followed by optimizer.step() which will update the model weights.\nFor model evaluation we define a validation loop which will calculate the F1 accuracy on the validation dataset. For validation we set our model to eval mode with model.eval() method. For calculating F1 accuracy, we use TorchMetrics which contains a collection of Machine Learning metrics for distributed, scalable PyTorch models and an easy-to-use API to create custom metrics.\n\n# EPOCH LOOP\nfor epoch in tqdm(range(1, num_epochs + 1)):\n\n    # TRAINING LOOP\n    model.train()\n    for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_ds)//batch_size):\n        data, target= data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if (batch_idx == 0) or ((batch_idx + 1) % log_interval == 0):\n            print(\n                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                    epoch,\n                    batch_idx * len(data),\n                    len(train_loader.dataset),\n                    100.0 * batch_idx / len(train_loader),\n                    loss.item(),\n                )\n            )\n            if dry_run:\n                break\n\n\n    # TESTING LOOP\n    model.eval()\n    test_loss = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data = data.to(device)\n            target = target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n\n            # WITH TorchMetrics\n            metric(output, target)\n\n            if dry_run:\n                break\n\n    # all_gather is used to aggregated the value across processes\n    test_loss = test_loss / len(val_loader.dataset)\n\n    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: ({metric.compute():.0f}%)\\n\")\n    metric.reset()\n\n    if dry_run:\n        break\n\n\n\n\n\n\n\nTrain Epoch: 1 [0/45929 (0%)]   Loss: 3.646841\n\nTest set: Average loss: 0.0008, Accuracy: (0%)"
  },
  {
    "objectID": "posts/2022-04-03-happywhale-pytorch-training-from-scratch-lite.html#happy-training",
    "href": "posts/2022-04-03-happywhale-pytorch-training-from-scratch-lite.html#happy-training",
    "title": "HappyWhale 🐳:PyTorch Training from scratch Lite ⚡️",
    "section": "Happy Training! ⚡️🎉",
    "text": "Happy Training! ⚡️🎉"
  },
  {
    "objectID": "posts/learning-rust/2022-11-29-learning-rust.html",
    "href": "posts/learning-rust/2022-11-29-learning-rust.html",
    "title": "My first day of learning Rust 🦀",
    "section": "",
    "text": "I have been using Python since my college (2018), it has allowed me to experiment my ideas quickly without worrying a lot about syntax and data structures. I could quickly implement complex algorithm and do some operations in a single line that would take more than 3 LOC in other languages like Java.\n# swapping variable in Python\na = 1\nb = 2\n\na, b = b, a\nToday, I am learning Rust. I am particularly interested in this programming language because of its use cases in wide areas like optimized image resize and creating optimized programs and binding it with other language like Python.\nNow let’s talk about Rust 🦀\nHow to create a dynamic string?\nWe can compare it with Python staticmethod SomeClass.somemethod, with :: instead of ..\n\nThe double colon :: operator allows us to namespace this particular from function under the String type rather than using some sort of name like string_from. (source)[https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html#the-string-type]\n\nlet s = String::from(\"hello\");\nThis string is mutable and to add more characters we can do the following -\n    s.push_str(\", world!\");"
  },
  {
    "objectID": "posts/stable-diffusion/2022-11-24 Stable Diffusion 2.html",
    "href": "posts/stable-diffusion/2022-11-24 Stable Diffusion 2.html",
    "title": "What is new in Stable Diffusion 2.0?",
    "section": "",
    "text": "While the world was already amazed by the performance of open source text to image model, Stable Diffusion 1.x. Stability AI has released a new version with a lot of improvements."
  },
  {
    "objectID": "posts/stable-diffusion/2022-11-24 Stable Diffusion 2.html#list-of-notable-updates",
    "href": "posts/stable-diffusion/2022-11-24 Stable Diffusion 2.html#list-of-notable-updates",
    "title": "What is new in Stable Diffusion 2.0?",
    "section": "List of notable updates:",
    "text": "List of notable updates:\n\nTrained using a new text encoder, OpenCLIP, developed by LAION with support from Stability AI.\nThe text-to-image models can now generate images with default resolutions of both 512x512 pixels and 768x768 pixels.\nThe models are trained on subset of LAION-5B dataset after filtering out adult content using NSFW filter.\nStable Diffusion 2.0 comes with an Upscaler Diffusion model that enhances the resolution of images by a factor of 4.\nDepth-to-Image: It extends the version 1 image-to-image by also considering depth of an input image for generation of a new image.\nIt also brings a new text guided image inpainting diffusion model, finetuned on the new Stable Diffusion 2.0 base text-to-image."
  },
  {
    "objectID": "posts/2020-07-26-fastapi-tf-webapp.html",
    "href": "posts/2020-07-26-fastapi-tf-webapp.html",
    "title": "Building Machine Learning API with FastAPI and Tensorflow",
    "section": "",
    "text": "Youtube tutorial version of this blog is also available \nFastAPI is a high-performance asynchronous framework for building APIs in Python. It provides support for Swagger UI out of the box.\n\nSource code for this blog is available aniketmaurya/tensorflow-fastapi-starter-pack\n\n\nhello-world of FastAPI\nFirst, we import FastAPI class and create an object app. This class has useful parameters like we can pass the title and description for Swagger UI.\nfrom fastapi import FastAPI\napp = FastAPI(title='Hello world', description='This is a hello world example', version='0.0.1')\nWe define a function and decorate it with @app.get. This means that our API /index supports the GET method. The function defined here is async, FastAPI automatically takes care of async and without async methods by creating a thread pool for the normal def functions and it uses an async event loop for async functions.\n@app.get('/index')\nasync def hello_world():\n    return \"hello world\"\n\n\nPydantic support\nOne of my favorite features offered by FastAPI is Pydantic support. We can define Pydantic models and request-response will be handled by FastAPI for these models. Let’s create a COVID-19 symptom checker API to understand this.\n\n\nCovid-19 symptom checker API\nWe create a request body, it is the format in which the client should send the request. It will be used by Swagger UI.\nfrom pydantic import BaseModel\n\nclass Symptom(BaseModel):\n    fever: bool = False\n    dry_cough: bool = False\n    tiredness: bool = False\n    breathing_problem: bool = False\nLet’s create a function to assign a risk level based on the inputs.\n\nThis is just for learning and should not be used in real life, better consult a doctor.\n\ndef get_risk_level(symptom: Symptom):\n    if not (symptom.fever or symptom.dry_cough or symptom.tiredness or symptom.breathing_problem):\n        return 'Low risk level. THIS IS A DEMO APP'\n    \n    if not (symptom.breathing_problem or symptom.dry_cough):\n        if symptom.fever:\n            return 'moderate risk level. THIS IS A DEMO APP'\n    \n    if symptom.breathing_problem:\n        return 'High-risk level. THIS IS A DEMO APP'\n    \n    return 'THIS IS A DEMO APP'\nLet’s create the API for checking the symptoms\n@app.post('/api/covid-symptom-check')\ndef check_risk(symptom: Symptom):\n    return get_risk_level(symptom)\n\n\nImage recognition API\nWe will create an API to classify images, we name it predict/image. We will use Tensorflow for creating the image classification model.\n\nTutorial for Image Classification with Tensorflow\n\nWe create a function load_model, which will return a MobileNet CNN Model with pre-trained weights i.e. it is already trained to classify 1000 unique categories of images.\nimport tensorflow as tf\n\ndef load_model():\n    model = tf.keras.applications.MobileNetV2(weights=\"imagenet\")\n    print(\"Model loaded\")\n    return model\n\nmodel = load_model()\nWe define a predict function that will accept an image and returns the predictions. We resize the image to 224x224 and normalize the pixel values to be in [-1, 1].\nfrom tensorflow.keras.applications.imagenet_utils import decode_predictions\ndecode_predictions is used to decode the class name of the predicted object. Here we will return the top-2 probable class.\ndef predict(image: Image.Image):\n\n    image = np.asarray(image.resize((224, 224)))[..., :3]\n    image = np.expand_dims(image, 0)\n    image = image / 127.5 - 1.0\n\n    result = decode_predictions(model.predict(image), 2)[0]\n\n    response = []\n    for i, res in enumerate(result):\n        resp = {}\n        resp[\"class\"] = res[1]\n        resp[\"confidence\"] = f\"{res[2]*100:0.2f} %\"\n\n        response.append(resp)\n\n    return response\nNow we will create an API /predict/image which supports file upload. We will filter the file extension to support only jpg, jpeg, and png format of images.\nWe will use Pillow to load the uploaded image.\ndef read_imagefile(file) -> Image.Image:\n    image = Image.open(BytesIO(file))\n    return image\n@app.post(\"/predict/image\")\nasync def predict_api(file: UploadFile = File(...)):\n    extension = file.filename.split(\".\")[-1] in (\"jpg\", \"jpeg\", \"png\")\n    if not extension:\n        return \"Image must be jpg or png format!\"\n    image = read_imagefile(await file.read())\n    prediction = predict(image)\n\n    return prediction\n\n\nFinal code\nimport uvicorn\nfrom fastapi import FastAPI, File, UploadFile\n\nfrom application.components import predict, read_imagefile\n\napp = FastAPI()\n\n@app.post(\"/predict/image\")\nasync def predict_api(file: UploadFile = File(...)):\n    extension = file.filename.split(\".\")[-1] in (\"jpg\", \"jpeg\", \"png\")\n    if not extension:\n        return \"Image must be jpg or png format!\"\n    image = read_imagefile(await file.read())\n    prediction = predict(image)\n\n    return prediction\n\n\n@app.post(\"/api/covid-symptom-check\")\ndef check_risk(symptom: Symptom):\n    return symptom_check.get_risk_level(symptom)\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, debug=True)\n\nFastAPI documentation is the best place to learn more about core concepts of the framework.\n\n\n > Hope you liked the article.\nFeel free to ask your questions in the comments or reach me out personally\n👉 Twitter: https://twitter.com/aniketmaurya\n👉 Linkedin: https://linkedin.com/in/aniketmaurya"
  },
  {
    "objectID": "posts/2022-11-20 Conditional variables.html",
    "href": "posts/2022-11-20 Conditional variables.html",
    "title": "AI with Aniket",
    "section": "",
    "text": "from threading import Condition, Lock, Thread\nimport random\n\n\nfound_divisible = False\nnum = None\ncond_var = Condition()\nexit_prog = False\n\n\ndef divisible(x):\n    return x%5==0\n\ndef finder():\n    global found_divisible, num, exit_prog\n    while not exit_prog:\n        x = random.randint(1, 1000)\n        if divisible(x):\n            cond_var.acquire()        \n            num = x\n            found_divisible = True\n            cond_var.notify()\n            cond_var.release()\n\n\ndef printer():\n    global num, found_divisible, exit_prog\n    while not exit_prog:\n        cond_var.acquire()\n        while not found_divisible and not exit_prog:\n            cond_var.wait()\n        print(num)\n        found_divisible = False\n        cond_var.release()\n\n\nprinterThread = Thread(target=printer)\nprinterThread.start()\n\nfinderThread = Thread(target=finder)\nfinderThread.start()\n\n525\n735\n930\n580\n100\n880\n320\n780\n720\n265\n965\n740\n375\n40\n835\n505\n120\n290\n985\n720\n745\n175\n\n\n\n# Let the threads run for 3 seconds\nimport time\ntime.sleep(3)\n\n# Let the threads exit\nexit_prog = True\n\ncond_var.acquire()\ncond_var.notifyAll()\ncond_var.release()\n\nprinterThread.join()\nfinderThread.join()\n\n370\n5\n520\n160\n835\n45\n490\n765\n265\n245\n605\n105\n90\n855\n775\n85\n760\n635\n565\n740\n845\n475\n655\n555\n705\n435\n920\n520\n970\n705\n770\n425\n530\n645\n380\n980\n650\n190\n640\n820\n785\n95\n110\n135\n815\n545\n330\n165\n950\n235\n575\n915\n145\n985\n515\n690\n775\n335\n260\n995\n165\n840\n315\n630\n825\n310\n915\n635\n885\n995\n250\n410\n470\n465\n395\n895\n990\n770\n360\n165\n470\n905\n385\n70\n905\n25\n765\n810\n915\n320\n110\n520\n775\n20\n415\n450\n15\n335\n495\n830\n615\n115\n300\n275\n475\n300\n450\n415\n445\n450\n500\n425\n370\n325\n670\n235\n1000\n650\n135\n340\n365\n605\n280\n520\n415\n675\n965\n180\n150\n580\n985\n85\n790\n835\n215\n550\n275\n285\n145\n5\n595\n340\n420\n885\n15\n695\n555\n890\n280\n630\n955\n550\n505\n470\n95\n595\n915\n440\n740\n335\n960\n400\n980\n245\n645\n995\n385\n190\n285\n585\n230\n330\n520\n915\n320\n225\n250\n755\n670\n995\n140\n430\n125\n195\n135\n745\n475\n655\n850\n295\n675\n735\n945\n155\n850\n335\n940\n235\n30\n370\n75\n55\n765\n645\n420\n940\n495\n520\n235\n410\n480\n465\n625\n610\n310\n615\n810\n355\n285\n290\n490\n350\n635\n785\n450\n235\n785\n80\n440\n90\n615\n365\n255\n310\n320\n90\n700\n85\n860\n950\n475\n670\n60\n15\n705\n30\n150\n810\n105\n920\n10\n40\n820\n610\n305\n75\n170\n135\n280\n150\n640\n405\n120\n410\n490\n575\n655\n580\n910\n545\n605\n25\n205\n195\n305\n145\n325\n850\n135\n505\n315\n835\n320\n555\n110\n250\n510\n745\n645\n915\n690\n835\n310\n385\n430\n450\n20\n30\n615\n330\n930\n685\n255\n130\n540\n305\n995\n440\n715\n585\n65\n870\n210\n640\n55\n720\n895\n410\n875\n600\n615\n400\n890\n210\n285\n740\n140\n675\n560\n360\n665\n500\n925\n545\n495\n590\n610\n320\n935\n710\n795\n405\n395\n80\n60\n610\n880\n955\n665\n840\n790\n715\n100\n875\n740\n675\n695\n645\n400\n25\n245\n560\n320\n940\n670\n165\n450\n610\n480\n220\n20\n560\n45\n550\n750\n135\n615\n65\n890\n120\n405\n90\n575\n495\n200\n590\n140\n310\n645\n685\n735\n10\n640\n400\n540\n155\n630\n240\n785\n300\n895\n920\n875\n645\n815\n345\n360\n925\n95\n890\n580\n630\n945\n255\n610\n260\n930\n885\n340\n300\n955\n830\n235\n890\n105\n40\n460\n315\n305\n955\n990\n395\n30\n850\n275"
  },
  {
    "objectID": "posts/2019-05-12-image-classification-with-tf2.html",
    "href": "posts/2019-05-12-image-classification-with-tf2.html",
    "title": "Image Classification with Tensorflow 2.x",
    "section": "",
    "text": "Image Classification with TF 2\nUnlike previous versions, TensorFlow 2.0 is coming out with some major changes. It is going to be more pythonic and no need to turn on eager execution explicitly. With tight integration of Keras now it will focus on simplicity and ease of use.\nKeras is a high-level API that allows to easily build, train, evaluate and execute all sorts of neural networks. Keras was developed by François Chollet and open-sourced in March 2015. With its simplicity and easy-to-use feature, it gained popularity very quickly. Tensorflow comes with its own implementation of Keras with some TF specific features. > Keras can run on top of MXNet, CNTK or Theano.\n\n\n\nTF-Logo\n\n\n\n\nBuilding a simple image classifier\nWe will create a simple Neural Networks architecture for image classification. Fashion MNIST is a collection of 70,000 grayscale images of 28x28 pixel each, with 10 classes of different clothing items. We will train our Neural Network on this dataset. > CNN performs better than Dense NN for image classification both in terms of time and accuracy. I have used Dense NN architecture here for demonstration.\n\nCheck this article to learn about Convolutional Neural Networks.\n\n\nImport libraries and download F-MNIST dataset.\nimport tensorflow as tf\nfrom tensorflow import keras  *# tensorflow implementation of keras*\nimport matplotlib.pyplot as plt\n\nDownload dataset with Keras utility function*\nfashion_mnist = keras.datasets.fashion_mnist\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n\nprint(X_train_full.shape)\n(60000, 28, 28)\nIt is always a good practice to split the dataset into training, validation and test set. Since we already have our test set so let’s create a validation set. We can scale the pixel intensities of the data to the 0–1 range by dividing 255.0. Scaling leads to better gradient update.\nX_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nWe can view any photo using matplotlib.\nplt.imshow(X_train[5])\n\n\n\n\nCreate a model using Keras Sequential API\nNow it’s the time to build our simple image classification Artificial Neural Networks.\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\nIf you didn’t get it then don’t worry, let me explain the code line by line.\nThe Sequential model is a linear stack of layers, connected sequentially.\nThe next layer, i.e. Flatten is just converting the 28x28 dimension array into a 1D array. If it receives input data X, then it computes X.reshape(-1, 1). It takes an input_shape argument to specify the size of the input data. However, input_shape can be automatically detected by Keras.\nThe** Dense **layer is the fully-connected neurons in the neural networks. Here, there are two hidden layers with 300 neurons in first and 100 neurons in the second hidden layer respectively.\nThe last Dense layer made up of 10 neurons in the output layer. It is responsible for calculating loss and predictions.\n\n\nCompiling the model\nKeras has a compile() method which specifies loss function to use, optimizer, and metrics.\nmodel.compile(loss=keras.losses.sparse_categorical_crossentropy,\noptimizer=\"sgd\", metrics=[\"accuracy\"])\n\n\nTrain and Evaluate\nAfter the model compilation, we can all fit() method by specifying the epochs, batch size, etc.\n\nTraining model\nhistory = model.fit(X_train, y_train,\nepochs=30, validation_data=(X_valid, y_valid*))*\nThis method will train the model for 30 epochs. Train loss, Validation loss and train accuracy, validation accuracy can be found in history.history.\n\n\nLoss visualization\nWe can create a visualization for the learning curve using history.\nimport pandas as pd\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1) *# set the vertical range to [0-1]*\nplt.show()\nSource: “Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow”\nWe can see that the space between validation and training curves are small that’s why there isn’t overfitting problem.\nNow, we can try out different hyperparameters to achieve more accuracy on the dataset.\n\n\nModel Evaluation\nIf you are satisfied with the training and validation accuracy then evaluate it on the test set.\n    model.evaluate(X_test, Y_test)\n\nAccuracy on test set might be lower than on validation set because the hyperparameters are tuned for validation set.\n\n\n\n\nSave the trained Model\nAfter you have trained and evaluated your NN model on test set you can download your model using Keras tf.keras.models.save_model method and then can load it anytime for inference.\ntf.keras.models.save_model(\"my_image_classifier\")\nIt saves both the model’s architecture and the value of all the model parameters for every layer (All trained weights and biases). This saved_model can be loaded to TF serving for deployement purpose.\nIf you want to use your trained model for inference, just load it:\nmodel = tf.keras.models.load_model(\"my_image_classifier\")\nNow, it’s time to train different datasets on your own. Good Luck 😄!\n\n # Recommended Resources\n\nDeep learning specialization (Coursera)\n“Hands-On Machine Learning with Scikit-Learn and TensorFlow” by Aurélien Géron (Book from O’Reilly)\n\n You can contact me at twitter.com/aniketmaurya or drop an 📧 at aniketmaurya@outlook.com"
  },
  {
    "objectID": "posts/2021-02-13-pix2pix explained with code.html",
    "href": "posts/2021-02-13-pix2pix explained with code.html",
    "title": "Pix2Pix - Image to image translation with Conditional Adversarial Networks",
    "section": "",
    "text": "Hi! My name is Aniket Maurya. I am a Machine Learning Engineer at Quinbay Technologies, India. I research and build ML products for an e-commerce giant. I like to share my limited knowledge of Machine Learning and Deep Learning with on my blog or YouTube channel. You can connect with me on Linkedin/Twitter."
  },
  {
    "objectID": "posts/2021-02-13-pix2pix explained with code.html#introduction-to-conditional-adversarial-networks",
    "href": "posts/2021-02-13-pix2pix explained with code.html#introduction-to-conditional-adversarial-networks",
    "title": "Pix2Pix - Image to image translation with Conditional Adversarial Networks",
    "section": "Introduction to Conditional Adversarial Networks",
    "text": "Introduction to Conditional Adversarial Networks\n\n\n\nPix2Pix example 01\n\n\nImage to Image translation means transforming the given source image into a different image. Gray scale image to colour image conversion is one such example of image of image translation.\nIn this tutorial we will discuss GANs, a few points from Pix2Pix paper and implement the Pix2Pix network to translate segmented facade into real pictures. We will create the Pix2Pix model in PyTorch and use PyTorch lightning to avoid boilerplates.\nGANs are Generative models that learns a mapping from random noise vector to an output image. G(z) -> Image (y)\nFor example, GANs can learn mapping from random normal vectors to generate smiley images. For training such a GAN we just need a set of smiley images and train the GAN with an adversarial loss 🙂. After the model is trained we can use random normal noise vectors to generate images that were not in the training dataset.\nBut what if we want to build a network in such a way that we can control what the model will generate. In our case we want the model to generate a laughing smiley.\nConditional GANs are Generative networks which learn mapping from random noise vectors and a conditional vector to output an image. Suppose we have 4 types of smileys - smile, laugh, sad and angry (🙂 😂 😔 😡). So our class vector for smile 🙂 can be (1,0,0,0), laugh can be 😂 (0,1,0,0) and similarly for others. Here the conditional vector is the smiley embedding.\nDuring training of the generator the conditional image is passed to the generator and fake image is generated. The fake image is then passed through the discriminator along with the conditional image, both fake image and conditional image are concatenated. Discriminator penalizes the generator if it correctly classifies the fake image as fake."
  },
  {
    "objectID": "posts/2021-02-13-pix2pix explained with code.html#pix2pix",
    "href": "posts/2021-02-13-pix2pix explained with code.html#pix2pix",
    "title": "Pix2Pix - Image to image translation with Conditional Adversarial Networks",
    "section": "Pix2Pix",
    "text": "Pix2Pix\nPix2Pix is an image-to-image translation Generative Adversarial Networks that learns a mapping from an image X and a random noise Z to output image Y or in simple language it learns to translate the source image into a different image.\nDuring the time Pix2Pix was released, several other works were also using Conditional GANs on discrete labels. Pix2Pix uses a U-Net based architecture for the Generator and for the Discriminator a PathGAN Classifier is used.\n\n\n\nPix2Pix Generator arch\n\n\nPix2Pix Generator is an U-Net based architecture which is an encoder-decoder network with skip connections. Both generator and discriminator uses Convolution-BatchNorm-ReLu like module or in simple words we can say that it is the unit block of the generator and discriminator. Skip connections are added between each layer i and layer n − i, where n is the total number of layers. At each skip connection all the channels from current layer i are concatenated with all the channels at n-i layer.\nLets understand more with code\n\n\nCode\nimport os\nfrom glob import glob\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pytorch_lightning as pl\nimport torch\nfrom PIL import Image\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import center_crop\nfrom torchvision.utils import make_grid\nfrom tqdm.auto import tqdm\n\n\nUncomment the below code to download the dataset\n\n# !wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n# !tar -xvf facades.tar.gz\n# http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n\nAfter downloading the dataset we create Dataloader which loads our conditional and real image.\n\npath = \"../../pytorch-gans/pix2pix/facades/train/\"\n\n\nclass FacadesDataset(Dataset):\n    def __init__(self, path, target_size=None):\n        self.filenames = glob(str(Path(path) / \"*\"))\n        self.target_size = target_size\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        filename = self.filenames[idx]\n        image = Image.open(filename)\n        image = transforms.functional.to_tensor(image)\n        image_width = image.shape[2]\n\n        real = image[:, :, : image_width // 2]\n        condition = image[:, :, image_width // 2 :]\n\n        target_size = self.target_size\n        if target_size:\n            condition = nn.functional.interpolate(condition, size=target_size)\n            real = nn.functional.interpolate(real, size=target_size)\n\n        return real, condition\n\nWe create the unit module that will be used in Generator and Discriminator (Convolution->BatchNorm->ReLu). We also keep our option open to use DropOut layer when we need.\n\nclass ConvBlock(nn.Module):\n    \"\"\"\n    Unit block of the Pix2Pix\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, use_dropout=False, use_bn=True):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n        if use_bn:\n            self.batchnorm = nn.BatchNorm2d(out_channels)\n        self.use_bn = use_bn\n\n        if use_dropout:\n            self.dropout = nn.Dropout()\n        self.use_dropout = use_dropout\n        self.activation = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        if self.use_bn:\n            x = self.batchnorm(x)\n        if self.use_dropout:\n            x = self.dropout(x)\n        x = self.activation(x)\n        return x\n\nIn the first part of U-Net network the layer size decreases, we create a DownSampleConv module for this. This module will contain the unit block that we just created ConvBlock.\n\nclass DownSampleConv(nn.Module):\n    def __init__(self, in_channels, use_dropout=False, use_bn=False):\n        super().__init__()\n\n        self.conv_block1 = ConvBlock(in_channels, in_channels * 2, use_dropout, use_bn)\n        self.conv_block2 = ConvBlock(\n            in_channels * 2, in_channels * 2, use_dropout, use_bn\n        )\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.maxpool(x)\n        return x\n\nNow in the second part the network expands and so we create UpSampleConv\n\nclass UpSampleConv(nn.Module):\n    def __init__(self, input_channels, use_dropout=False, use_bn=True):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n        self.conv1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=2)\n        self.conv2 = nn.Conv2d(\n            input_channels, input_channels // 2, kernel_size=3, padding=1\n        )\n        self.conv3 = nn.Conv2d(\n            input_channels // 2, input_channels // 2, kernel_size=2, padding=1\n        )\n        if use_bn:\n            self.batchnorm = nn.BatchNorm2d(input_channels // 2)\n        self.use_bn = use_bn\n        self.activation = nn.ReLU()\n        if use_dropout:\n            self.dropout = nn.Dropout()\n        self.use_dropout = use_dropout\n\n    def forward(self, x, skip_con_x):\n\n        x = self.upsample(x)\n        x = self.conv1(x)\n        skip_con_x = center_crop(skip_con_x, x.shape[-2:])\n        x = torch.cat([x, skip_con_x], axis=1)\n        x = self.conv2(x)\n        if self.use_bn:\n            x = self.batchnorm(x)\n        if self.use_dropout:\n            x = self.dropout(x)\n        x = self.activation(x)\n        x = self.conv3(x)\n        if self.use_bn:\n            x = self.batchnorm(x)\n        if self.use_dropout:\n            x = self.dropout(x)\n        x = self.activation(x)\n        return x\n\nNow the basic blocks of the Pix2Pix generated is created, we create the generator module. Generator is formed of expanding and contracting layers. The first part network contracts and then expands again, i.e. first we have encoder block and then decoder block. Below is the encoder-decoder of U-Net network configuration from official paper. Here C denotes the unit block that we created ConvBlock and D denotes Drop Out with value 0.5. In the decoder, the output tensors from n-i layer of encoder concatenates with i layer of the decoder. Also the first three blocks of the decoder has drop out layers.\nEncoder:  C64-C128-C256-C512-C512-C512-C512-C512\n\nDecoder:  CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels, out_channels, hidden_channels=32, depth=6):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=1)\n\n        self.conv_final = nn.Conv2d(hidden_channels, out_channels, kernel_size=1)\n        self.depth = depth\n\n        self.contracting_layers = []\n        self.expanding_layers = []\n        self.sigmoid = nn.Sigmoid()\n\n        # encoding/contracting path of the Generator\n        for i in range(depth):\n            down_sample_conv = DownSampleConv(\n                hidden_channels * 2 ** i,\n            )\n            self.contracting_layers.append(down_sample_conv)\n\n        # decoder/Expanding path of the Generator\n        for i in range(depth):\n            upsample_conv = UpSampleConv(\n                hidden_channels * 2 ** (i + 1), use_dropout=(True if i < 3 else False)\n            )\n            self.expanding_layers.append(upsample_conv)\n\n        self.contracting_layers = nn.ModuleList(self.contracting_layers)\n        self.expanding_layers = nn.ModuleList(self.expanding_layers)\n\n    def forward(self, x):\n        depth = self.depth\n        contractive_x = []\n\n        x = self.conv1(x)\n        contractive_x.append(x)\n\n        for i in range(depth):\n            x = self.contracting_layers[i](x)\n            contractive_x.append(x)\n\n        for i in range(depth - 1, -1, -1):\n            x = self.expanding_layers[i](x, contractive_x[i])\n        x = self.conv_final(x)\n\n        return self.sigmoid(x)\n\n\nDiscriminator\nA discriminator is a ConvNet which learns to classify images into discrete labels. In GANs, discriminators learns to predict whether the given image is real or fake. PatchGAN is the discriminator used for Pix2Pix. Its architecture is different from a typical image classification ConvNet because of the output layer size. In convnets output layer size is equal to the number of classes while in PatchGAN output layer size is a 2D matrix.\nNow we create our Discriminator - PatchGAN. In this network we use the same DownSampleConv module that we created for generator.\n\nclass PatchGAN(nn.Module):\n\n    def __init__(self, input_channels, hidden_channels=8):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, hidden_channels, kernel_size=1)\n        self.contract1 = DownSampleConv(hidden_channels, use_bn=False)\n        self.contract2 = DownSampleConv(hidden_channels * 2)\n        self.contract3 = DownSampleConv(hidden_channels * 4)\n        self.contract4 = DownSampleConv(hidden_channels * 8)\n        self.final = nn.Conv2d(hidden_channels * 16, 1, kernel_size=1)\n\n    def forward(self, x, y):\n        x = torch.cat([x, y], axis=1)\n        x0 = self.conv1(x)\n        x1 = self.contract1(x0)\n        x2 = self.contract2(x1)\n        x3 = self.contract3(x2)\n        x4 = self.contract4(x3)\n        xn = self.final(x4)\n        return xn\n\n\n\nLoss Function\nLoss function used in Pix2Pix are Adversarial loss and Reconstruction loss. Adversarial loss is used to penalize the generator to predict more realistic images. In conditional GANs, generators job is not only to produce realistic image but also to be near the ground truth output. Reconstruction Loss helps network to produce the realistic image near the conditional image.\nadversarial_loss = nn.BCEWithLogitsLoss()\n\nreconstruction_loss = nn.L1Loss()\n\n\nCode\n# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n\ndef _weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\n\n\n\nclass Pix2Pix(pl.LightningModule):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=32,\n                 depth=6,\n                 learning_rate=0.0002,\n                 lambda_recon=200):\n\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.gen = Generator(in_channels, out_channels, hidden_channels, depth)\n        self.patch_gan = PatchGAN(in_channels + out_channels, hidden_channels=8)\n\n        # intializing weights\n        self.gen = self.gen.apply(_weights_init)\n        self.patch_gan = self.patch_gan.apply(_weights_init)\n\n        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n        self.recon_criterion = nn.L1Loss()\n\n    def _gen_step(self, real_images, conditioned_images):\n        # Pix2Pix has adversarial and a reconstruction loss\n        # First calculate the adversarial loss\n        fake_images = self.gen(conditioned_images)\n        disc_logits = self.patch_gan(fake_images, conditioned_images)\n        adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits))\n\n        # calculate reconstruction loss\n        recon_loss = self.recon_criterion(fake_images, real_images)\n        lambda_recon = self.hparams.lambda_recon\n\n        return adversarial_loss + lambda_recon * recon_loss\n\n    def _disc_step(self, real_images, conditioned_images):\n        fake_images = self.gen(conditioned_images).detach()\n        fake_logits = self.patch_gan(fake_images, conditioned_images)\n\n        real_logits = self.patch_gan(real_images, conditioned_images)\n\n        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n        return (real_loss + fake_loss) / 2\n\n    def configure_optimizers(self):\n        lr = self.hparams.learning_rate\n        gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr)\n        disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr)\n        return disc_opt, gen_opt\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        real, condition = batch\n\n        loss = None\n        if optimizer_idx == 0:\n            loss = self._disc_step(real, condition)\n            self.log('PatchGAN Loss', loss)\n        elif optimizer_idx == 1:\n            loss = self._gen_step(real, condition)\n            self.log('Generator Loss', loss)\n        \n        if self.current_epoch%50==0 and batch_idx==0 and optimizer_idx==1:\n            fake = self.gen(condition).detach()\n            show_tensor_images(condition[0])\n            show_tensor_images(real[0])\n            show_tensor_images(fake[0])\n        return loss\n\nNow that the network is implemented now we are ready to train. You can also modify the dataloader and train on custom dataset.\nHope you liked the article! Happy training 😃\n\ndataset = FacadesDataset(path, target_size=target_size)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\npix2pix = Pix2Pix(3, 3)\ntrainer = pl.Trainer(max_epochs=100, gpus=1)\ntrainer.fit(pix2pix, dataloader)"
  },
  {
    "objectID": "posts/2020-10-15-deploy-python-heroku.html",
    "href": "posts/2020-10-15-deploy-python-heroku.html",
    "title": "Deploy Machine Learning Web Apps for Free",
    "section": "",
    "text": "Do you want to deploy your Python code to web for free? Read this post to know the step-by-step procedure to host your web app for free on Heroku cloud.\n\nDeploying a Machine Learning model is a difficult task due to the requirement of large memory and powerful computation. This tutorial focuses on a simple deployment technique that can be used to deploy any Python web app for free.\n\nRead my previous article to learn how to build an “Image classification web app with FastAPI and Tensorflow”\n\nIf you prefer a video tutorial then click on the video below:\n\n\n\n\n\nStep 1\nFirst of all, you will need a Heroku id, so go now and register for a free account.\nFor deploying any Python app on Heroku, we need three files- requirements.txt, runtime.txt, and Procfile.\nrequirements.txt is a normal text file that contains Python packages required to run the app.\nruntime.txt is a text file that will contain the Python Version you want your app to run on.\nProcfile will contain the command to launch your web app. For example, you can use\nmethod 1\n`python application/server/main.py`\n\nor uvicorn if you are deploying a uvicorn server\n`uvicorn application.server.main:app`\n\n\nStep 2\nGo to your Heroku dashboard then click on New and create a new app\n\n\n\n Enter your App name and select the Server region that is nearest to your location and click on Create app\n\n\n\n # Step 3 After you create the app, you will see the deployment methods — Heroku Git, GitHub, and Container Registry. I will use the GitHub method. For this just push your code repository to your GitHub account and then connect to GitHub on Heroku.\n\nThen search the repository and connect it to your Heroku app.\n\n\n\nAfter this, you will see a deploy button, select the branch of your Git repository that you want to be deployed, and click on deploy.\nThen Heroku will automatically start your deployment 🎉🎉 After deployment, you can access your app from any web browser 🔥\n\n\nHope this article helped you in the deployment of your web app. If you have some feedback or suggestion please let me know in the comment section.\n\nFollow me on Twitter: https://twitter.com/aniketmaurya\n\n\nSubscribe to my YouTube channel: https://www.youtube.com/channel/UCRuFsj94hWecPkuEr4f5Xww"
  },
  {
    "objectID": "posts/2022-04-10 torchdata.html",
    "href": "posts/2022-04-10 torchdata.html",
    "title": "TorchData: PyTorch Data loading utility library",
    "section": "",
    "text": "In this tutorial, we will learn about TorchData\n\n\n\nPhoto by Scott Webb from Pexels\n\n\nPyTorch 1.11 came with a new libray called TorchData. It provides common data loading primitives for easily constructing flexible and performant data pipelines. TorchData promotes composable data loading for code reusablity with DataPipes.\nDataPipes is the building block of TorchData and works out of the box with PyTorch DataLoader. It can be chained to form a data pipeline where data will be transformed from each DataPipe.\nFor example if we have image dataset in a folder with a CSV mapping of classes and we want to create a DataLoader that returns batch of image Tensor and labels.\nFor this we need we do the following steps:\n\nRead and Parse the CSV\n\nGet image filepath\nDecode label\n\nRead Image\nConvert Image to Tensor\nReturn image Tensor and Label index\n\nThese steps can be a chained DataPipe where the initial data will flow from the first step to the very last applying transformations in each step.\nNow, lets see how to do the same with TorchData code.\n\n!pip install torchdata -q\n\n\nimport torch\nfrom torchdata.datapipes.iter import (\n    FileOpener,\n    Filter,\n    FileLister, Filter, \n)\n\n\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms.functional import to_tensor\n\nWe will use CIFAR-10 dataset which has the same structure as we discussed.\nFrom TorchData docs:\n\nWe have implemented over 50 DataPipes that provide different core functionalities, such as opening files, parsing texts, transforming samples, caching, shuffling, and batching. For users who are interested in connecting to cloud providers (such as Google Drive or AWS S3), the fsspec and iopath DataPipes will allow you to do so. The documentation provides detailed explanations and usage examples of each IterDataPipe and MapDataPipe.\n\nBasically, TorchData has already implemented 50 DataPipes which you can use directly. Here we will use use FileOpener and parse_csv to read the csv data.\n\nROOT = \"/Users/aniket/datasets/cifar-10/train\"\n\ncsv_dp = FileLister(f\"{ROOT}/../trainLabels.csv\")\ncsv_dp = FileOpener(csv_dp)\ncsv_dp = csv_dp.parse_csv()\n\nfor i, e in enumerate(csv_dp):\n    if i>10: break\n    print(e)\n\n['id', 'label']\n['1', 'frog']\n['2', 'truck']\n['3', 'truck']\n['4', 'deer']\n['5', 'automobile']\n['6', 'automobile']\n['7', 'bird']\n['8', 'horse']\n['9', 'ship']\n['10', 'cat']\n\n\nWe don’t need the header of csv in our datapipe ([id, label]), so we will use the inbuilt Filter DataPipe to remove it.\n\ncsv_dp = Filter(csv_dp, lambda x: x[1]!=\"label\")\nlabels = {e: i for i, e in enumerate(set([e[1] for e in csv_dp]))}\n\nfor i, e in enumerate(csv_dp):\n    if i>4: break\n    print(e)\n\n['1', 'frog']\n['2', 'truck']\n['3', 'truck']\n['4', 'deer']\n['5', 'automobile']\n\n\nNow, we have a DataPipe csv_dp which flows file id and label. We need to conver the file id into filepath and label in label index.\nWe can map functions to the DataPipe and even form a chain of mapping to apply transformations.\n\ndef get_filename(data):    \n    idx, label = data\n    return f\"{ROOT}/{idx}.png\", label\n\ndp = csv_dp.map(get_filename)\nfor i, e in enumerate(dp):\n    if i>4: break\n    print(e)\n\n('/Users/aniket/datasets/cifar-10/train/1.png', 'frog')\n('/Users/aniket/datasets/cifar-10/train/2.png', 'truck')\n('/Users/aniket/datasets/cifar-10/train/3.png', 'truck')\n('/Users/aniket/datasets/cifar-10/train/4.png', 'deer')\n('/Users/aniket/datasets/cifar-10/train/5.png', 'automobile')\n\n\n\nfrom IPython.display import display\n\ndef load_image(data):\n    file, label = data\n    return Image.open(file), label\n\ndp = dp.map(load_image)\n\nfor i, e in enumerate(dp):\n    display(e[0])\n    print(e[1])\n    if i>=5: break\n\n\n\n\nfrog\n\n\n\n\n\ntruck\n\n\n\n\n\ntruck\n\n\n\n\n\ndeer\n\n\n\n\n\nautomobile\n\n\n\n\n\nautomobile\n\n\nFinally we map the datapipe to process image to Tensor and label to index.\n\ndef process(data):\n    img, label = data\n    return to_tensor(img), labels[label]\n\ndp = dp.map(process)\n\nIf you have come this far then I have a bonus for you. Train an image classifier using DataPipe and PyTorch Lightning Flash ⚡️\nFlash expects the dataloader to be in form of a dictionary with keys input and target where input will contain our image tensor and target will be the label index.\n\ndp = dp.map(lambda x: {\"input\": x[0], \"target\": x[1]})\n\nAs we discussed that DataPipes are fully compatible with DataLoader so this is how you convert a DataPipe to DataLoader.\n\ndl = DataLoader(\n        dp,\n        batch_size=32,\n        shuffle=True,\n    )\n\nTraining an Image Classifier with Flash is super easy. Flash provides Deep Learning tasks based APIs that you can use to train your model. Currently, our task is image classification so let’s import the ImageClassifier and build our model.\n\nfrom flash.image import ImageClassifier\nimport flash\n\n\nmodel = ImageClassifier(num_classes=len(labels), backbone=\"efficientnet_b0\", pretrained=False)\n\nUsing 'efficientnet_b0' provided by rwightman/pytorch-image-models (https://github.com/rwightman/pytorch-image-models).\n\n\n\n# Create the trainer and finetune the model\ntrainer = flash.Trainer(max_epochs=3)\n\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n\ntrainer.fit(model, dl)\n\n\n  | Name          | Type           | Params\n-------------------------------------------------\n0 | train_metrics | ModuleDict     | 0     \n1 | val_metrics   | ModuleDict     | 0     \n2 | test_metrics  | ModuleDict     | 0     \n3 | adapter       | DefaultAdapter | 4.0 M \n-------------------------------------------------\n4.0 M     Trainable params\n0         Non-trainable params\n4.0 M     Total params\n16.081    Total estimated model params size (MB)"
  },
  {
    "objectID": "posts/2019-01-07-face-recognition.html",
    "href": "posts/2019-01-07-face-recognition.html",
    "title": "Face Recognition",
    "section": "",
    "text": "AI is revolutionizing the world. Face recognition is one such spectrum of it. Almost everyone uses face recognition systems — on our mobile, Facebook, Photo gallery apps or advanced security cameras. Learn how these systems are able to recognize our faces.\nThis article is inspired by the deeplearning.ai course on FaceNet."
  },
  {
    "objectID": "posts/2019-01-07-face-recognition.html#face-verification-vs.-face-recognition",
    "href": "posts/2019-01-07-face-recognition.html#face-verification-vs.-face-recognition",
    "title": "Face Recognition",
    "section": "Face Verification vs. Face Recognition",
    "text": "Face Verification vs. Face Recognition\nFace Verification checks “is this the claimed person?”. For example, in school, you go with your ID card and the invigilator verifies your face with the ID card. This is Face Verification. A mobile phone that unlocks using our face is also using face verification. It is 1:1 matching problem.\nNow suppose the invigilator knows everyone by their name. So, you decide to go there without an ID card. The invigilator identifies your face and lets you in. This is Face Recognition. Face Recognition deals with “who is this person?” problem. We can say that it is a 1:K problem."
  },
  {
    "objectID": "posts/2019-01-07-face-recognition.html#why-pixel-by-pixel-comparison-of-images-is-a-bad-idea",
    "href": "posts/2019-01-07-face-recognition.html#why-pixel-by-pixel-comparison-of-images-is-a-bad-idea",
    "title": "Face Recognition",
    "section": "Why pixel-by-pixel comparison of images is a bad idea?",
    "text": "Why pixel-by-pixel comparison of images is a bad idea?\nA simple way for face verification can be comparing two images pixel-by-pixel and if the threshold between images is less than a threshold then we can say that they’re the same person. But since the pixel values in an image change dramatically even with a slight change of light, position or orientation. So, this method doesn’t work well."
  },
  {
    "objectID": "posts/2019-01-07-face-recognition.html#face-embedding-come-to-the-rescue",
    "href": "posts/2019-01-07-face-recognition.html#face-embedding-come-to-the-rescue",
    "title": "Face Recognition",
    "section": "Face Embedding come to the rescue",
    "text": "Face Embedding come to the rescue\nThe embedding is represented by f (x), a d-dimensional vector. It encodes an image ‘x’ into a d-dimensional Euclidean space. The face embedding of two images of the same person are similar to each other and that of different persons are very different.\nIn ConvNet architectures, the initial layers learn to recognize basic patterns like straight lines, edges, circles, etc. and the deeper layers learn to recognize more complex patterns like numbers, faces, cars, etc.\nTo get our embedding we feed the image into a pre-trained model then run a forward propagation and extract the vector from some deeper Fully-Connected layer. You can learn the basics of CNN here.\n\nHow to finetune embeddings?\nThe Triplet Loss function takes the responsibility to push the embeddings of two images of the same person (Anchor and Positive) closer together while pulling the embeddings of two images of different persons (Anchor, Negative) further apart.\nSource: Coursera\n\\(\\|\\|f(X1) — f(X2)\\|\\|²\\) is the degree of similarity between image X1 and X2. If it’s smaller than a chosen threshold then both are the same person.\nIf you’re wondering “what this \\(\\|A\\|\\) weird symbol is?”, it’s called Frobenius Norm.\nThe distance between Anchor and Positive images should be less and the distance between Anchor and Negative images should be high.\ni.e. \\(∥f(Anchor) — f(Positive)∥² ≤ ∥f(Anchor) — f(Negative)∥²\\).\nTraining a Face recognition model is computationally expensive so it’s recommended to download a pre-trained model.\nStart with creating a database of persons containing an embedding vector for each.\n#create a dictionary database\ndb = dict()\n#encoding(image_path) converts image to embedding\ndb['person1'] = encoding('person1.jpg')\ndb['person2'] = encoding('person2.jpg')\ndb['person3'] = encoding('person3.jpg')\ndb['person4'] = encoding('person4.jpg')\ndb['person5'] = encoding('person5.jpg')"
  },
  {
    "objectID": "posts/2019-01-07-face-recognition.html#face-verification",
    "href": "posts/2019-01-07-face-recognition.html#face-verification",
    "title": "Face Recognition",
    "section": "Face Verification",
    "text": "Face Verification\nNow that we have created our database, we can define a function that accepts image embedding and name of the person as the argument and it will verify if they are the same person.\ndef verify(embedding, person_name):\n        \n    # numpy.linalg.norm calculates the Frobenius Norm\n    dist = np.linalg.norm(embedding - db[person_name])\n    \n    # Chosen threshold is 0.7  \n    if dist < 0.7:\n        print(\"Verified! Welcome \" + person_name)\n    else:\n        print(\"Person name and face didn't match!\")\nHurray 😎! We created our Face Verification system. Now let’s create the Face Recognition System. If you remember, a person doesn’t need any ID in the face recognition system. He just needs to show his face to the camera."
  },
  {
    "objectID": "posts/2019-01-07-face-recognition.html#face-recognition",
    "href": "posts/2019-01-07-face-recognition.html#face-recognition",
    "title": "Face Recognition",
    "section": "Face Recognition",
    "text": "Face Recognition\nIn Face recognition, the distance will be calculated for all the images in the database against the input embedding and the smallest distance has to be searched.\ndef recognize_me(input_embedding):\n\n    # Set min_dist to infinity\n    min_dist = np.inf\n    \n    # Iterate over the database to calulate distance for each person*\n    for (name, emb) in db.items():\n        \n        # Compute the distance*\n        curr_dist = np.linalg.norm(input_embedding - emb)\n\n        # identity is set to the name of the person from the database whose distance is smallest against the input encoding\n        if curr_dist < min_dist:\n            min_dist = curr_dist\n            identity = name\n    \n    if min_dist > 0.7:\n        print(\"Sorry! You’re not in the database.\")\n    else:\n        print (\"Hi! Welcome \" + identity)\n\nCongratulations!! 👏👏 You have created your own Face Recognition system.\n\nYou can provide your feedback in comment section below.\nFollow me on Twitter"
  },
  {
    "objectID": "posts/2020-04-08-tf.data-Creating-Data-Input-Pipelines.html",
    "href": "posts/2020-04-08-tf.data-Creating-Data-Input-Pipelines.html",
    "title": "tf.data: Creating data input pipelines",
    "section": "",
    "text": "Are you not able to load your NumPy data into memory? Does your model have to wait for data to be loaded after each epoch? Is your Keras DataGenerator slow?\nTensorFlow tf.data API allows building complex input pipelines. It easily handles a large amount of data and can read different formats of data while allowing complex data transformations."
  },
  {
    "objectID": "posts/2020-04-08-tf.data-Creating-Data-Input-Pipelines.html#why-do-we-need-tf.data",
    "href": "posts/2020-04-08-tf.data-Creating-Data-Input-Pipelines.html#why-do-we-need-tf.data",
    "title": "tf.data: Creating data input pipelines",
    "section": "Why do we need tf.data?",
    "text": "Why do we need tf.data?\nA training step involves the following steps: 1. File reading 2. Fetch or parse data 3. Data transformation 4. Using the data to train the model.\nsource: Tensorflow\nIf you have a large amount of data and you’re unable to load it into the memory, you may want to use Generators. But Generators has limited portability and scalability.\nAfter every epoch, you will wait for data to be transformed into a consumable format by the model and during that time your model sits idle, not doing any training. This leads to low CPU and GPU utilization.\nOne solution to handle this is to prefetch your data in advance and you won’t have to wait for data to be loaded.\nsource: Tensorflow\ntf.data is a data input pipeline building API than you can use to easily build your data pipeline. Whether you want to read data from local files or even if your data is stored remotely."
  },
  {
    "objectID": "posts/2020-04-08-tf.data-Creating-Data-Input-Pipelines.html#loading-data-for-classification",
    "href": "posts/2020-04-08-tf.data-Creating-Data-Input-Pipelines.html#loading-data-for-classification",
    "title": "tf.data: Creating data input pipelines",
    "section": "Loading data for classification",
    "text": "Loading data for classification\nTo train an image classification model, we create a CNN model and feed our data to the model. I want to train a Cats vs Dogs classifier and my data is stored in the following folder structure.\ndata\n└── train\n    ├── cat -> contains images of cats\n    └── dog -> contains images of dogs*\nWe first find the path of all the images-\nfrom glob import glob\nimport tensorflow as tf\n\nimage_path_list = glob('data/train/*/*.jpg')\ndata = tf.data.Dataset.list_files(image_path_list)\ntf.data.Dataset.list_files converts the list returned by glob method to the Dataset object. Now, we will load the images and their class.\ndef load_images(path):\n    \n    image = tf.io.read_file(path)\n    image = tf.io.decode_image(image)\n    label = tf.strings.split(path, os.path.sep)[-2]\n\n    return image, label\n\ndata = data.map(load_images)\nSo, the data object now has images and labels. But this is not it, we will have to resize the image, preprocess and apply transformations.\ndef preprocess(image, label):\n    image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n    image /= 255.\n    image -= 0.5\n\n    return image, label\n\ndata = data.map(preprocess)\nI have created a small library named Chitra, based on tf.data that can be used to skip all these steps.\nfrom chitra import dataloader as dl\npath = './data/train'\n\ntrain_dl = dl.Clf()\ndata = train_dl.from_folder(path, target_shape=(224, 244), shuffle = True)\n\n# to visualize the data\ntrain_dl.show_batch(6, figsize=(6,6))\nYou can just specify the path of your data and it will be loaded with the target size.\n > You can find my code at https://github.com/aniketmaurya/chitra"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Random Thoughts\n\n\n\n\n\nRandom Thoughts\n\n\n\n\n\n\nFeb 6, 2023\n\n\nAniket Maurya\n\n\n\n\n\n\n  \n\n\n\n\nMy first day of learning Rust 🦀\n\n\n\n\n\n\n\nrust\n\n\nnotes\n\n\n\n\nDocumentation of learning Rust\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWhat is new in Stable Diffusion 2.0?\n\n\n\n\n\n\n\ntext-to-image\n\n\nimage generation\n\n\n\n\nWhat is new in Stable Diffusion 2.0? It’s trained with a brand new text encoder OpenCLIP and Depth-to-Image diffusion model.\n\n\n\n\n\n\nNov 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTorchData: PyTorch Data loading utility library\n\n\n\n\n\n\n\npytorch\n\n\n\n\nLearn How to load image data with TorchData and train Image classifier\n\n\n\n\n\n\nApr 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHappyWhale 🐳:PyTorch Training from scratch Lite ⚡️\n\n\n\n\n\n\n\nkaggle\n\n\n\n\nLearn how to write a custom training loop in pure PyTorch, create custom torch Dataset class, compute metrics for model performance, and Scale the Training on any hardware like GPU, TPU, IPU or Distributed Training with LightningLite.\n\n\n\n\n\n\nApr 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPix2Pix - Image to image translation with Conditional Adversarial Networks\n\n\n\n\n\n\n\ngans\n\n\npytorch\n\n\n\n\nA tutorial on Pix2Pix Conditional GANs and implementation with PyTorch\n\n\n\n\n\n\nFeb 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nDCGAN Tutorial - Generate Fake Celebrity image\n\n\n\n\n\n\n\nGANs\n\n\n\n\nA beginner-friendly tutorial on DCGAN with PyTorch to generate Fake celebrity images with CelebA dataset.\n\n\n\n\n\n\nNov 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\nDeploy Machine Learning Web Apps for Free\n\n\n\n\n\n\n\npython\n\n\n\n\nLearn to deploy Python Tensorflow & FastAPI Web app on Heroku Cloud in 5 minutes.\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Machine Learning API with FastAPI and Tensorflow\n\n\n\n\n\n\n\npython\n\n\n\n\nTutorial on FastAPI - high performance asynchronous framework for faster development of production ready APIs.\n\n\n\n\n\n\nJul 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\nProbability vs Likelihood vs Maximum Likelihood\n\n\n\n\n\n\n\nStatistics\n\n\nMachine Learning\n\n\n\n\nTheory and examples of Probability vs Likelihood vs Maximum Likelihood\n\n\n\n\n\n\nApr 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\nNormalization\n\n\n\n\n\n\n\nDeep Learning\n\n\nMachine Learning\n\n\n\n\nNormalization techniques and effect of normalization.\n\n\n\n\n\n\nApr 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\ntf.data: Creating data input pipelines\n\n\n\n\n\n\n\nTensorflow\n\n\n\n\nBuilding scalabale data input pipeline with Tensorflow tf.data.\n\n\n\n\n\n\nApr 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression from Scratch\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nA tutorial on Linear Regression from scratch in Python\n\n\n\n\n\n\nMar 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\nEfficientDet: When Object Detection Meets Scalability and Efficiency\n\n\n\n\n\n\n\nObject Detection\n\n\n\n\nEfficientDet, highly efficient and scalable state of the art object detection model developed by Google Research, Brain Team.\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\nImage Classification with Tensorflow 2.x\n\n\n\n\n\n\n\nTensorflow\n\n\nDeep Learning\n\n\n\n\nImage Classification with Tensorflow 2.x.\n\n\n\n\n\n\nMay 12, 2019\n\n\n\n\n\n\n  \n\n\n\n\nFace Recognition\n\n\n\n\n\n\n\nTensorflow\n\n\nFace Recognition\n\n\n\n\nLearn to create Face Recognition system from scratch\n\n\n\n\n\n\nJan 7, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "👋 I’m a Developer Advocate at Lightning AI 🥑⚡️.\nI build Intelligent Software with ML and create/maintain some cool open-source ML/DL/Python libraries.\nI’ve created Gradsflow and Chitra - Python Libraries with aim to simplify AutoML and MLOps.\nI love building problem solving products that leverage Artificial Intelligence. I’ve expertise in building and deploying scalable Deep Learning system in production. I’ve worked on complete end-to-end process of Deep Learning i.e. Data creation, Model Training, API Development to Deployment.\nI believe in continuous learning, open-source and knowledge sharing. Started my YouTube channel to share educational video tutorials and tech Meetup on Machine Learning and Python Software development.\nContact me to talk about ML/AI, freelancing or consulation."
  },
  {
    "objectID": "posts/random/2023-02-06 random.html",
    "href": "posts/random/2023-02-06 random.html",
    "title": "Random Thoughts",
    "section": "",
    "text": "Nothingness"
  }
]